{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: many_stop_words in c:\\users\\dell\\appdata\\roaming\\python\\python37\\site-packages (0.2.2)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import codecs\n",
    "import nltk\n",
    "import re\n",
    "import pprint\n",
    "from nltk.corpus import stopwords\n",
    "!pip install --user many_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from many_stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_file_paths = sorted(glob.glob(\"*.txt\"))\n",
    "nltk_words=list(stopwords.words('english'))\n",
    "many_stop_words=list(get_stop_words('en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rawBook_to_processed(abook):\n",
    "    rawbookstring = u''\n",
    "    with codecs.open(abook, \"r\",) as book_file:\n",
    "        rawbookstring += book_file.read()\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    \n",
    "    \n",
    "    raw_sentences = tokenizer.tokenize(rawbookstring)\n",
    "    \n",
    "    nltk_words = list(stopwords.words('english'))\n",
    "\n",
    "    def sentence_to_wordlist(raw):\n",
    "        clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n",
    "        words = clean.split()\n",
    "        lowercase = [word.strip().lower() for word in words]\n",
    "        filtered_word_list = lowercase[:]  # make a copy of the word_list\n",
    "        \n",
    "        for word in lowercase:  # iterate over word_list\n",
    "            if word in many_stop_words: \n",
    "                filtered_word_list.remove(word) #\n",
    "        return filtered_word_list\n",
    "    \n",
    "    words = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            words.append(sentence_to_wordlist(raw_sentence))\n",
    "    return words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1 = rawBook_to_processed(books_file_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in words1 for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for afile in books_file_paths:\n",
    "    books.append ( [item.lower() for sublist in rawBook_to_processed(afile) for item in sublist] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(286 unique tokens: ['areas', 'bad', 'beriberi', 'big', 'buy']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(books)\n",
    "dictionary.save('./LDApickles/Got.dict')  # store the dictionary, for future reference\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'areas': 0, 'bad': 1, 'beriberi': 2, 'big': 3, 'buy': 4, 'called': 5, 'chemicals': 6, 'children': 7, 'contaminated': 8, 'destroy': 9, 'die': 10, 'disease': 11, 'eat': 12, 'famine': 13, 'food': 14, 'foods': 15, 'growing': 16, 'health': 17, 'hungry': 18, 'kwashiorkor': 19, 'long': 20, 'metals': 21, 'microorganisms': 22, 'money': 23, 'obese': 24, 'overweight': 25, 'people': 26, 'problem': 27, 'problems': 28, 'protein': 29, 'rickets': 30, 'scurvy': 31, 'shortage': 32, 'sick': 33, 'starvation': 34, 'thiamine': 35, 'time': 36, 'today': 37, 'vitamin': 38, 'water': 39, 'weather': 40, 'answer': 41, 'chocolate': 42, 'considered': 43, 'delight': 44, 'documented': 45, 'experts': 46, 'gods': 47, 'good': 48, 'happy': 49, 'history': 50, 'holds': 51, 'magic': 52, 'quote': 53, 'scroll': 54, 'summarizes': 55, 'sweet': 56, 'wondering': 57, 'abuse': 58, 'academic': 59, 'achieving': 60, 'age': 61, 'announced': 62, 'area': 63, 'arunachal': 64, 'attention': 65, 'authority': 66, 'bachelor': 67, 'bihar': 68, 'captivated': 69, 'career': 70, 'category': 71, 'central': 72, 'child': 73, 'class': 74, 'compared': 75, 'compulsory': 76, 'conclusion': 77, 'consist': 78, 'counted': 79, 'country': 80, 'courses': 81, 'crime': 82, 'decided': 83, 'degree': 84, 'department': 85, 'derp': 86, 'differ': 87, 'difference': 88, 'diploma': 89, 'distinguished': 90, 'district': 91, 'educated': 92, 'education': 93, 'elementary': 94, 'enhance': 95, 'essay': 96, 'exploitation': 97, 'facilities': 98, 'female': 99, 'focused': 100, 'future': 101, 'global': 102, 'goal': 103, 'god': 104, 'government': 105, 'graduation': 106, 'great': 107, 'helps': 108, 'high': 109, 'higher': 110, 'highest': 111, 'huge': 112, 'improve': 113, 'increased': 114, 'india': 115, 'infrastructure': 116, 'interesting': 117, 'jharkhand': 118, 'kerala': 119, 'labour': 120, 'lack': 121, 'launched': 122, 'learning': 123, 'level': 124, 'life': 125, 'listed': 126, 'literacy': 127, 'literate': 128, 'lowest': 129, 'majorly': 130, 'male': 131, 'manner': 132, 'marks': 133, 'master': 134, 'medium': 135, 'number': 136, 'pattern': 137, 'percentage': 138, 'period': 139, 'person': 140, 'phd': 141, 'point': 142, 'poor': 143, 'pradesh': 144, 'previous': 145, 'primary': 146, 'priority': 147, 'professional': 148, 'program': 149, 'proper': 150, 'prove': 151, 'quality': 152, 'rate': 153, 'ratio': 154, 'recover': 155, 'referred': 156, 'required': 157, 'result': 158, 'revitalisation': 159, 'running': 160, 'rural': 161, 'schools': 162, 'secondary': 163, 'select': 164, 'shares': 165, 'situation': 166, 'skill': 167, 'small': 168, 'social': 169, 'stage': 170, 'story': 171, 'stream': 172, 'streams': 173, 'student': 174, 'students': 175, 'study': 176, 'subjects': 177, 'successful': 178, 'survey': 179, 'teachers': 180, 'teaching': 181, 'training': 182, 'turning': 183, 'urban': 184, 'uttar': 185, 'viewed': 186, 'warming': 187, 'years': 188, 'young': 189, 'ability': 190, 'activities': 191, 'activity': 192, 'admitted': 193, 'admitting': 194, 'aim': 195, 'annual': 196, 'arisf': 197, 'arranged': 198, 'arranging': 199, 'association': 200, 'athleticism': 201, 'attempting': 202, 'based': 203, 'bona': 204, 'breaking': 205, 'bridge': 206, 'cases': 207, 'casual': 208, 'champion': 209, 'checkers': 210, 'chess': 211, 'claim': 212, 'classification': 213, 'committee': 214, 'compete': 215, 'competing': 216, 'competitions': 217, 'competitive': 218, 'consecutively': 219, 'contest': 220, 'contestants': 221, 'contests': 222, 'council': 223, 'definition': 224, 'definitions': 225, 'dexterity': 226, 'draughts': 227, 'draw': 228, 'element': 229, 'enjoyment': 230, 'ensure': 231, 'entertainment': 232, 'europe': 233, 'exceed': 234, 'exist': 235, 'federation': 236, 'fide': 237, 'forms': 238, 'games': 239, 'generally': 240, 'hundreds': 241, 'includes': 242, 'individuals': 243, 'international': 244, 'largest': 245, 'leagues': 246, 'limits': 247, 'loser': 248, 'maintain': 249, 'major': 250, 'match': 251, 'meeting': 252, 'methods': 253, 'mind': 254, 'olympic': 255, 'organisations': 256, 'organised': 257, 'participants': 258, 'participation': 259, 'physical': 260, 'playoffs': 261, 'precluding': 262, 'producing': 263, 'provide': 264, 'providing': 265, 'racing': 266, 'recognised': 267, 'recognises': 268, 'recognition': 269, 'regular': 270, 'season': 271, 'sides': 272, 'simultaneous': 273, 'simultaneously': 274, 'single': 275, 'skills': 276, 'spectators': 277, 'sport': 278, 'sportaccord': 279, 'sports': 280, 'teams': 281, 'tie': 282, 'tournament': 283, 'winner': 284, 'xiangqi': 285}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 3), (2, 1), (3, 1), (4, 1), (5, 4), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 4), (12, 7), (13, 1), (14, 8), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 8), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 3), (34, 1), (35, 1), (36, 1), (37, 1), (38, 3), (39, 1), (40, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.doc2bow(books[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models.ldamodel as lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = lda.LdaModel(corpus, id2word=dictionary, num_topics=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = mymodel.print_topic(88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.003*\"committee\" + 0.003*\"recognises\" + 0.003*\"compete\" + 0.003*\"pradesh\" + 0.003*\"classification\" + 0.003*\"claim\" + 0.003*\"chess\" + 0.003*\"checkers\" + 0.003*\"champion\" + 0.003*\"casual\" + 0.003*\"cases\" + 0.003*\"bridge\" + 0.003*\"breaking\" + 0.003*\"bona\" + 0.003*\"based\" + 0.003*\"attempting\" + 0.003*\"athleticism\" + 0.003*\"association\" + 0.003*\"arranging\" + 0.003*\"arranged\" + 0.003*\"arisf\" + 0.003*\"annual\" + 0.003*\"regular\" + 0.003*\"aim\" + 0.003*\"mind\" + 0.003*\"organisations\" + 0.003*\"major\" + 0.003*\"sports\" + 0.003*\"teams\" + 0.003*\"tie\" + 0.003*\"tournament\" + 0.003*\"winner\" + 0.003*\"spectators\" + 0.003*\"sportaccord\" + 0.003*\"recognition\" + 0.003*\"meeting\" + 0.003*\"recognised\" + 0.003*\"racing\" + 0.003*\"providing\" + 0.003*\"provide\" + 0.003*\"producing\" + 0.003*\"precluding\" + 0.003*\"playoffs\" + 0.003*\"physical\" + 0.003*\"participation\" + 0.003*\"participants\" + 0.003*\"organised\" + 0.003*\"olympic\" + 0.003*\"admitting\" + 0.003*\"admitted\" + 0.003*\"activity\" + 0.003*\"shares\" + 0.003*\"select\" + 0.003*\"secondary\" + 0.003*\"schools\" + 0.003*\"rural\" + 0.003*\"running\" + 0.003*\"revitalisation\" + 0.003*\"result\" + 0.003*\"required\" + 0.003*\"referred\" + 0.003*\"recover\" + 0.003*\"ratio\" + 0.003*\"rate\" + 0.003*\"quality\" + 0.003*\"prove\" + 0.003*\"proper\" + 0.003*\"program\" + 0.003*\"professional\" + 0.003*\"priority\" + 0.003*\"primary\" + 0.003*\"competing\" + 0.003*\"situation\" + 0.003*\"skill\" + 0.003*\"small\" + 0.003*\"social\" + 0.003*\"activities\" + 0.003*\"ability\" + 0.003*\"young\" + 0.003*\"years\" + 0.003*\"warming\" + 0.003*\"viewed\" + 0.003*\"uttar\" + 0.003*\"urban\" + 0.003*\"turning\" + 0.003*\"training\" + 0.003*\"sport\" + 0.003*\"teaching\" + 0.003*\"survey\" + 0.003*\"successful\" + 0.003*\"subjects\" + 0.003*\"study\" + 0.003*\"students\" + 0.003*\"student\" + 0.003*\"streams\" + 0.003*\"stream\" + 0.003*\"story\" + 0.003*\"stage\" + 0.003*\"teachers\" + 0.003*\"simultaneously\"'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel.print_topic(10, topn=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.003*\"young\" + 0.003*\"warming\" + 0.003*\"admitting\" + 0.003*\"admitted\" + 0.003*\"activity\" + 0.003*\"activities\" + 0.003*\"ability\" + 0.003*\"annual\" + 0.003*\"years\" + 0.003*\"urban\"'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'0.003*\"young\" + 0.003*\"warming\" + 0.003*\"admitting\" + 0.003*\"admitted\" + 0.003*\"activity\" + 0.003*\"activities\" + 0.003*\"ability\" + 0.003*\"annual\" + 0.003*\"years\" + 0.003*\"urban\"'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var.encode('ascii','replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
